- why we want to use kubernetes is, incase some failures happes to our workloads or microservices this kubernets able to re create that resource.
- we have created a part some time back and let me try to delete it and we will see whether Kubernetes could able to create this resource again or not.
```sh
[root@ip-172-31-26-3 tmp]# kubectl delete pod demo-pod
pod "demo-pod" deleted
[root@ip-172-31-26-3 tmp]#
```
- If we delete or if pod get terminated, there is no way it is going to recreate by default. So how we can overcome this problem?
- Yes, of course, we need to do some changes to the way of using our manifest files. That is where we can use deployments.
- we have tried to deploy few applications by using the kubectl command with the deployment option. Same thing we can use it as a manifest file.

```sh
vi regapp-deployment.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: shiv-regapp
  labels:
    app: regapp

spec:
  replicas: 2
  selector: 
    matchLabels:
      app: regapp

  template:
    metadata:
      labels:
        app: regapp
    spec:
      containers:
      - name: regapp
        image: shivangidodia1/regapp    (here mentioned the same image earlier we have pushed on DockerHub)
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
```
- For deployments, we should use apps/v1.The kind is deployment (Earlier we were using pod or service) Now I can say this entire snippet is going to tell us that what is our deployment name and deployment label. You can see here this is the deployment name that is shiv reg-app and the label.
- This deployment need a pod, right? Because whenever we execute deployment, it is going to create pods in the backend. So this template is going to tell you that how to create a pod and what pod it has to launch.
- So in this, again, we have the pod definition, then this pod internally requires a container. This is going to tell the container definition. So container definition, nothing but from where it can pull the image. And what is the image pulling policy? Nothing but whenever we execute this deployment file, it is always going to pull the latest image from our GitHub repository. next, on which port we can expose this container application.
```sh
  template:
    metadata:
      labels:
        app: regapp
    spec:
      containers:
      - name: regapp
        image: shiv/regapp
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
```

- By using this information, it is going to create a replica set. That is the reason you can see here match label app, register app, with this label weshould have a pod definition.
```sh
  template:
    metadata:
      labels:
        app: regapp
```
- I have given also a strategy that is rolling update, which represents that in case there is a new image in our GitHub, how it is going to create a pod with the new image.So let's take that here we have two replicas. It won't delete both. At a time, it is going to delete one container and create a new container with a new image. It will make sure that the new container is serving the application properly. Then it is going to terminate second one and create the new container with the latest image. That is how we will make sure that our application is not going down.
- But for this deployment, we need a service file, right?

```sh
apiVersion: v1
kind: Service
metadata:
  name: shiv-service
  labels:
    app: regapp

spec:
  selector:
    app: regapp

  ports:
  - name: regapp-port
    port: 8080
    targetPort: 8080

  type: LoadBalancer
```
- apiVersion and kind tells us that what kind of resource we are creating.
- Next thing, metadata, what is the service name we are using and what is the label.
- Next specification, here we are using selector, nothing but which deployment it should select. So this selector information and deployment label should match. And the next thing, it is talking about the load balancer. We are using service type as a load balancer. And on which port number we are exposing our service at cluster level.
- Next target port, whenever a new request comes to our load balancer, on which port number it is going to talk with the container.

- make sure that the label, whatever you have given over here, app, register app, should match with the selector in the service file.
![label selector](https://github.com/user-attachments/assets/ce6035ff-4352-4311-a0e0-a2041a53a013)

- Also, make sure that the container port, whatever you are using over here, make sure that same port number we are using here. So that the service is going to send the request to the container port on the same port number.
![port](https://github.com/user-attachments/assets/fecedd39-d49f-4919-b825-78684a1bc2e2)















