- why we want to use kubernetes is, incase some failures happes to our workloads or microservices this kubernets able to re create that resource.
- we have created a part some time back and let me try to delete it and we will see whether Kubernetes could able to create this resource again or not.
```sh
[root@ip-172-31-26-3 tmp]# kubectl delete pod demo-pod
pod "demo-pod" deleted
[root@ip-172-31-26-3 tmp]#
```
- If we delete or if pod get terminated, there is no way it is going to recreate by default. So how we can overcome this problem?
- Yes, of course, we need to do some changes to the way of using our manifest files. That is where we can use deployments.
- we have tried to deploy few applications by using the kubectl command with the deployment option. Same thing we can use it as a manifest file.

```sh
vi regapp-deployment.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: shiv-regapp
  labels:
    app: regapp

spec:
  replicas: 2
  selector: 
    matchLabels:
      app: regapp

  template:
    metadata:
      labels:
        app: regapp
    spec:
      containers:
      - name: regapp
        image: shivangidodia1/regapp    (here mentioned the same image earlier we have pushed on DockerHub)
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
```
- For deployments, we should use apps/v1.The kind is deployment (Earlier we were using pod or service) Now I can say this entire snippet is going to tell us that what is our deployment name and deployment label. You can see here this is the deployment name that is shiv reg-app and the label.
- This deployment need a pod, right? Because whenever we execute deployment, it is going to create pods in the backend. So this template is going to tell you that how to create a pod and what pod it has to launch.
- So in this, again, we have the pod definition, then this pod internally requires a container. This is going to tell the container definition. So container definition, nothing but from where it can pull the image. And what is the image pulling policy? Nothing but whenever we execute this deployment file, it is always going to pull the latest image from our GitHub repository. next, on which port we can expose this container application.
```sh
  template:
    metadata:
      labels:
        app: regapp
    spec:
      containers:
      - name: regapp
        image: shiv/regapp
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
```

- By using this information, it is going to create a replica set. That is the reason you can see here match label app, register app, with this label weshould have a pod definition.
```sh
  template:
    metadata:
      labels:
        app: regapp
```
- I have given also a strategy that is rolling update, which represents that in case there is a new image in our GitHub, how it is going to create a pod with the new image.So let's take that here we have two replicas. It won't delete both. At a time, it is going to delete one container and create a new container with a new image. It will make sure that the new container is serving the application properly. Then it is going to terminate second one and create the new container with the latest image. That is how we will make sure that our application is not going down.
- But for this deployment, we need a service file, right?

```sh
apiVersion: v1
kind: Service
metadata:
  name: shiv-service
  labels:
    app: regapp

spec:
  selector:
    app: regapp

  ports:
  - name: regapp-port
    port: 8080
    targetPort: 8080

  type: LoadBalancer
```
- apiVersion and kind tells us that what kind of resource we are creating.
- Next thing, metadata, what is the service name we are using and what is the label.
- Next specification, here we are using selector, nothing but which deployment it should select. So this selector information and deployment label should match. And the next thing, it is talking about the load balancer. We are using service type as a load balancer. And on which port number we are exposing our service at cluster level.
- Next target port, whenever a new request comes to our load balancer, on which port number it is going to talk with the container.

- make sure that the label, whatever you have given over here, app, register app, should match with the selector in the service file.
![label selector](https://github.com/user-attachments/assets/ce6035ff-4352-4311-a0e0-a2041a53a013)

- Also, make sure that the container port, whatever you are using over here, make sure that same port number we are using here. So that the service is going to send the request to the container port on the same port number.
![port](https://github.com/user-attachments/assets/fecedd39-d49f-4919-b825-78684a1bc2e2)


#### Now create and access pod using deployment and service manifect file
```sh
kubectl apply -f regapp-deployment.yml
```

- to verify
```sh
[root@ip-172-31-26-3 tmp]# kubectl apply -f service.yml 
service/shiv-service created
[root@ip-172-31-26-3 tmp]#
```
```sh
[root@ip-172-31-26-3 tmp]# kubectl get all
NAME                               READY   STATUS    RESTARTS   AGE
pod/shiv-regapp-6965f78b4b-2lm78   1/1     Running   0          11m
pod/shiv-regapp-6965f78b4b-kv58b   1/1     Running   0          11m
pod/shiv-regapp-6965f78b4b-txbkw   0/1     Pending   0          11m

NAME                   TYPE           CLUSTER-IP       EXTERNAL-IP                                                              PORT(S)          AGE
service/kubernetes     ClusterIP      10.100.0.1       <none>                                                                   443/TCP          147m
service/shiv-service   LoadBalancer   10.100.215.193   ac7e1a214879341c085ff657ee4af935-807641946.us-east-1.elb.amazonaws.com   8080:31566/TCP   10m

NAME                          READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/shiv-regapp   2/3     3            2           11m

NAME                                     DESIRED   CURRENT   READY   AGE
replicaset.apps/shiv-regapp-6965f78b4b   3         3         2       11m
[root@ip-172-31-26-3 tmp]#
```


- here when i checked 1 pod was nt running and shows pending. i checked by describe pod and found that maximum number of pod they can hendle those are already running means 2 pods only capacity.
- 
```sh
[root@ip-172-31-26-3 tmp]# kubectl describe pod shiv-regapp-6965f78b4b-txbkw
Name:             shiv-regapp-6965f78b4b-txbkw
Namespace:        default
Priority:         0
Service Account:  default
Node:             <none>
Labels:           app=regapp
                  pod-template-hash=6965f78b4b
Annotations:      <none>
Status:           Pending
IP:               
IPs:              <none>
Controlled By:    ReplicaSet/shiv-regapp-6965f78b4b
Containers:
  regapp:
    Image:        shivangidodia1/regapp
    Port:         8080/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gbb92 (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  kube-api-access-gbb92:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Warning  FailedScheduling  4m19s (x3 over 14m)  default-scheduler  0/2 nodes are available: 2 Too many pods. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod.
[root@ip-172-31-26-3 tmp]#
```

- So I scale down the deployment to 2 replicas then its worked
```sh
[root@ip-172-31-26-3 tmp]# kubectl scale deployment shiv-regapp --replicas=2
deployment.apps/shiv-regapp scaled
[root@ip-172-31-26-3 tmp]# kubectl get all
NAME                               READY   STATUS    RESTARTS   AGE
pod/shiv-regapp-6965f78b4b-2lm78   1/1     Running   0          18m
pod/shiv-regapp-6965f78b4b-kv58b   1/1     Running   0          18m

NAME                   TYPE           CLUSTER-IP       EXTERNAL-IP                                                              PORT(S)          AGE
service/kubernetes     ClusterIP      10.100.0.1       <none>                                                                   443/TCP          154m
service/shiv-service   LoadBalancer   10.100.215.193   ac7e1a214879341c085ff657ee4af935-807641946.us-east-1.elb.amazonaws.com   8080:31566/TCP   16m

NAME                          READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/shiv-regapp   2/2     2            2           18m

NAME                                     DESIRED   CURRENT   READY   AGE
replicaset.apps/shiv-regapp-6965f78b4b   2         2         2       18m
[root@ip-172-31-26-3 tmp]#
```

- Now if i checked loadbalancer fqdna:8080 tomcat webapp we can access, also we can /webbapp page that is our application.
- this means we could deploy our application on pod
- In case if any pod get deleted, it should be reprovision right?
```sh
[root@ip-172-31-26-3 tmp]# kubectl get pod
NAME                           READY   STATUS    RESTARTS   AGE
shiv-regapp-6965f78b4b-2lm78   1/1     Running   0          24m
shiv-regapp-6965f78b4b-kv58b   1/1     Running   0          24m
[root@ip-172-31-26-3 tmp]#
```
```sh
[root@ip-172-31-26-3 tmp]# kubectl delete pod shiv-regapp-6965f78b4b-2lm78
pod "shiv-regapp-6965f78b4b-2lm78" deleted
[root@ip-172-31-26-3 tmp]#
```
- it is provisioning new pod
```sh
[root@ip-172-31-26-3 tmp]# kubectl get pod
NAME                           READY   STATUS              RESTARTS   AGE
shiv-regapp-6965f78b4b-5f6rr   0/1     ContainerCreating   0          28s
shiv-regapp-6965f78b4b-kv58b   1/1     Running             0          25m
[root@ip-172-31-26-3 tmp]# 
```


- Replicaset make sure that we have 2 pods at any point of time 

